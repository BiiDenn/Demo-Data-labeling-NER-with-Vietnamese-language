import streamlit as st
import pandas as pd
import re
import pytesseract
from PIL import Image
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
import spacy 

# Cáº¥u hÃ¬nh Tesseract (náº¿u cÃ i á»Ÿ vá»‹ trÃ­ khÃ¡c, hÃ£y chá»‰nh láº¡i Ä‘Æ°á»ng dáº«n nÃ y)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

st.set_page_config(page_title="AI Legal Text & NER Demo", page_icon="âš–ï¸", layout="wide")

st.title("âš–ï¸ á»¨NG Dá»¤NG Há»ŒC SÃ‚U TRONG Xá»¬ LÃ VÄ‚N Báº¢N PHÃP LÃ & RÃšT TRÃCH THá»°C THá»‚ TIáº¾NG VIá»†T")

# =========================
# Táº¡o Tabs
# =========================
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "ğŸ·ï¸ GÃ¡n nhÃ£n dá»¯ liá»‡u",
    "ğŸ” NER Rule-based (OCR)",
    "ğŸ“ˆ NER Classical (Statistical)",
    "ğŸ§  NER Deep Learning (BiLSTM-CRF)",
    "ğŸ¤– NER Transformer (OCR + PhoBERT/Electra)"
])


# ======================
# TAB 1 â€” Data Labeling
# ======================
with tab1:
    st.header("ğŸ·ï¸ GÃN NHÃƒN Dá»® LIá»†U (Data Labeling Demo)")
    st.markdown("Minh há»a quÃ¡ trÃ¬nh **gÃ¡n nhÃ£n dá»¯ liá»‡u thá»§ cÃ´ng** cho cÃ¡c máº«u email â€” bÆ°á»›c ná»n táº£ng cho mÃ´ hÃ¬nh AI há»c cÃ³ giÃ¡m sÃ¡t.")

    emails = [
        "TÃ i khoáº£n cá»§a báº¡n Ä‘Ã£ bá»‹ khÃ³a, vui lÃ²ng Ä‘Äƒng nháº­p táº¡i http://abc.com Ä‘á»ƒ xÃ¡c nháº­n.",
        "Khuyáº¿n mÃ£i 50% cho Ä‘Æ¡n hÃ ng hÃ´m nay! NgoÃ i ra, báº¥m vÃ o http://abc.com Ä‘á»ƒ nháº­n thÆ°á»Ÿng $1000.",
        "Xin chÃ o, Ä‘Ã¢y lÃ  hÃ³a Ä‘Æ¡n thÃ¡ng 10 cá»§a báº¡n.",
        "Vui lÃ²ng xÃ¡c minh thÃ´ng tin ngÃ¢n hÃ ng cá»§a báº¡n táº¡i Ä‘Æ°á»ng dáº«n dÆ°á»›i Ä‘Ã¢y."
    ]
    true_labels = ["ğŸš¨ Spam/Giáº£ máº¡o", "ğŸš¨ Spam/Giáº£ máº¡o", "âœ… BÃ¬nh thÆ°á»ng", "ğŸš¨ Spam/Giáº£ máº¡o"]

    if "labels" not in st.session_state:
        st.session_state.labels = [None] * len(emails)

    for i, email in enumerate(emails):
        st.write(f"**Email {i+1}:** {email}")
        label = st.radio(
            f"Chá»n nhÃ£n cho email {i+1}",
            ["ChÆ°a gÃ¡n", "ğŸš¨ Spam/Giáº£ máº¡o", "âœ… BÃ¬nh thÆ°á»ng"],
            key=f"email_{i}"
        )
        if label != "ChÆ°a gÃ¡n":
            st.session_state.labels[i] = label

    if st.button("ğŸ“Š Hiá»ƒn thá»‹ káº¿t quáº£ gÃ¡n nhÃ£n"):
        user_labels = st.session_state.labels
        comparison = ["âœ… ÄÃºng" if user_labels[i] == true_labels[i] else "âŒ Sai"
                      for i in range(len(emails))]

        df = pd.DataFrame({
            "Email": emails,
            "NhÃ£n ngÆ°á»i gÃ¡n": user_labels,
            "NhÃ£n Ä‘Ãºng": true_labels,
            "ÄÃ¡nh giÃ¡": comparison
        })

        def highlight_row(row):
            color = "#d4edda" if row["ÄÃ¡nh giÃ¡"] == "âœ… ÄÃºng" else "#f8d7da"
            return [f"background-color: {color}"] * len(row)

        st.dataframe(df.style.apply(highlight_row, axis=1), use_container_width=True)

        correct = sum(1 for c in comparison if c == "âœ… ÄÃºng")
        acc = correct / len(comparison) * 100
        st.success(f"ğŸ¯ Äá»™ chÃ­nh xÃ¡c gÃ¡n nhÃ£n: **{acc:.1f}%**")

# ============================
# TAB 2 â€” Rule-based with OCR
# ============================
with tab2:
    st.header("ğŸ” RÃšT TRÃCH THá»°C THá»‚ â€” HÆ¯á»šNG 1: Rule-based")
    st.markdown("""
    Báº¡n cÃ³ thá»ƒ **táº£i hÃ¬nh áº£nh vÄƒn báº£n phÃ¡p lÃ½ (JPG/PNG)** hoáº·c **nháº­p trá»±c tiáº¿p ná»™i dung vÄƒn báº£n** Ä‘á»ƒ há»‡ thá»‘ng trÃ­ch xuáº¥t tÃªn ngÆ°á»i, tá»• chá»©c, Ä‘á»‹a Ä‘iá»ƒm.
    """)

    mode = st.radio("Chá»n phÆ°Æ¡ng thá»©c nháº­p dá»¯ liá»‡u:", ["ğŸ“„ Táº£i áº£nh", "âœï¸ Nháº­p vÄƒn báº£n"], key="mode_rule")

    extracted_text = ""

    if mode == "ğŸ“„ Táº£i áº£nh":
        uploaded_img = st.file_uploader("ğŸ“ Chá»n áº£nh vÄƒn báº£n phÃ¡p lÃ½ (JPG/PNG)", type=["jpg", "jpeg", "png"], key="img_rule")
        if uploaded_img is not None:
            img = Image.open(uploaded_img)
            st.image(img, caption="ğŸ“œ áº¢nh vÄƒn báº£n Ä‘Æ°á»£c táº£i lÃªn", use_column_width=True)
            with st.spinner("ğŸ”  Äang Ä‘á»c ná»™i dung vÄƒn báº£n tá»« áº£nh (OCR)..."):
                extracted_text = pytesseract.image_to_string(img, lang="vie")

    else:
        extracted_text = st.text_area("âœï¸ Nháº­p ná»™i dung vÄƒn báº£n táº¡i Ä‘Ã¢y:", height=250)

    if st.button("ğŸ” RÃºt trÃ­ch thá»±c thá»ƒ (Rule-based)"):
        if not extracted_text.strip():
            st.warning("âš ï¸ Vui lÃ²ng táº£i áº£nh hoáº·c nháº­p ná»™i dung vÄƒn báº£n trÆ°á»›c.")
        else:
            # --- Regex nháº­n dáº¡ng thá»±c thá»ƒ ---
            pattern_person = r"\b([A-ZÄ][a-zÃ Ã¡áº£Ã£áº¡Äƒáº±áº¯áº³áºµáº·Ã¢áº§áº¥áº©áº«áº­Ä‘Ãªá»áº¿á»ƒá»…á»‡Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Æ°á»«á»©á»­á»¯á»±]+(?:\s[A-ZÄ][a-zÃ Ã¡áº£Ã£áº¡Äƒáº±áº¯áº³áºµáº·Ã¢áº§áº¥áº©áº«áº­Ä‘Ãªá»áº¿á»ƒá»…á»‡Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Æ°á»«á»©á»­á»¯á»±]+){1,3})\b"
            pattern_org = r"\b(Há»c\s?viá»‡n\s[A-ZÄa-z\s]+|PhÃ²ng\s[A-ZÄa-z\-]+\s?[A-ZÄa-z\-]*|TrÆ°á»ng\s[A-ZÄa-z\s]+|CÃ´ng\s?[Tt]y\s[A-ZÄa-z\s]+|Bá»™\s[A-ZÄa-z\s]+|Sá»Ÿ\s[A-ZÄa-z\s]+|á»¦y\sban\s[A-ZÄa-z\s]+)\b"
            pattern_loc = r"\b(TP\.?\s?Há»“\s?ChÃ­\s?Minh|HÃ \s?Ná»™i|ÄÃ \s?Náºµng|Huáº¿|Quáº­n\s?\d+)\b"

            persons = re.findall(pattern_person, extracted_text)
            orgs = re.findall(pattern_org, extracted_text)
            locs = re.findall(pattern_loc, extracted_text)

            # --- Highlight káº¿t quáº£ ---
            highlighted = extracted_text
            for p in persons:
                highlighted = highlighted.replace(p, f"<span style='color:green; font-weight:bold'>{p}</span>")
            for o in orgs:
                highlighted = highlighted.replace(o, f"<span style='color:orange; font-weight:bold'>{o}</span>")
            for l in locs:
                highlighted = highlighted.replace(l, f"<span style='color:purple; font-weight:bold'>{l}</span>")

            st.markdown("### ğŸ§© Káº¿t quáº£ trÃ­ch xuáº¥t (Rule-based):", unsafe_allow_html=True)
            st.markdown(highlighted, unsafe_allow_html=True)
            st.write("**ğŸ‘¤ NgÆ°á»i (PER):**", persons)
            st.write("**ğŸ¢ Tá»• chá»©c (ORG):**", orgs)
            st.write("**ğŸ“ Äá»‹a Ä‘iá»ƒm (LOC):**", locs)


# =========================
# TAB 3 â€” Classical NER (English spaCy)
# =========================
with tab3:
    st.header("ğŸ“ˆ RÃšT TRÃCH THá»°C THá»‚ â€” HÆ¯á»šNG 2A: Classical (spaCy - English)")
    st.markdown("""
    Sá»­ dá»¥ng model **spaCy `en_core_web_sm`** (tiáº¿ng Anh) Ä‘á»ƒ minh há»a phÆ°Æ¡ng phÃ¡p thá»‘ng kÃª (statistical NER).  
    > MÃ´ hÃ¬nh nÃ y dá»±a trÃªn Ä‘áº·c trÆ°ng ngá»¯ phÃ¡p (POS, dependency, shape, prefix/suffix) vÃ  dÃ¹ng CRF-like decoder Ä‘á»ƒ gÃ¡n nhÃ£n chuá»—i.
    """)

    # Ã” nháº­p vÄƒn báº£n
    text_stat = st.text_area(
        "âœï¸ Nháº­p vÄƒn báº£n cáº§n trÃ­ch xuáº¥t:",
        "Barack Obama was born in Hawaii and worked at the White House.",
        height=150
    )

    # NÃºt cháº¡y
    if st.button("ğŸ” RÃºt trÃ­ch thá»±c thá»ƒ (spaCy English)"):
        try:
            nlp = spacy.load("en_core_web_sm")  # âœ… Model tiáº¿ng Anh cÃ³ sáºµn
        except OSError:
            st.error("""
            â— Model `en_core_web_sm` chÆ°a Ä‘Æ°á»£c cÃ i.
            CÃ i nhanh trong terminal:
            ```bash
            python -m spacy download en_core_web_sm
            ```
            """)
            nlp = None

        if nlp is not None:
            # PhÃ¢n tÃ­ch vÄƒn báº£n
            doc = nlp(text_stat)
            ents = [(ent.text, ent.label_) for ent in doc.ents]

            if not ents:
                st.warning("âš ï¸ KhÃ´ng phÃ¡t hiá»‡n thá»±c thá»ƒ nÃ o trong vÄƒn báº£n nÃ y.")
            else:
                # Highlight
                highlighted = text_stat
                for text, label in ents:
                    color = (
                        "green" if label in ["PERSON"] else
                        "orange" if label in ["ORG"] else
                        "purple" if label in ["GPE", "LOC"] else
                        "blue"
                    )
                    highlighted = highlighted.replace(
                        text,
                        f"<span style='color:{color}; font-weight:bold'>{text}</span>"
                    )

                st.markdown("### ğŸ§© Káº¿t quáº£ trÃ­ch xuáº¥t (spaCy English):", unsafe_allow_html=True)
                st.markdown(highlighted, unsafe_allow_html=True)

                # Báº£ng káº¿t quáº£
                df = pd.DataFrame(ents, columns=["Thá»±c thá»ƒ", "Loáº¡i"])
                st.dataframe(df, use_container_width=True)

            with st.expander("Giáº£i thÃ­ch thÃªm"):
                st.markdown("""
                - `PERSON`: TÃªn ngÆ°á»i  
                - `ORG`: Tá»• chá»©c / cÃ´ng ty  
                - `GPE` hoáº·c `LOC`: Äá»‹a Ä‘iá»ƒm, quá»‘c gia, thÃ nh phá»‘  
                - `FAC`: CÆ¡ sá»Ÿ váº­t cháº¥t
                - `DATE`, `MONEY`, `TIME`: NgÃ y, tiá»n, thá»i gian  
                
                ÄÃ¢y lÃ  vÃ­ dá»¥ minh há»a Classical NER dá»±a trÃªn Ä‘áº·c trÆ°ng thá»‘ng kÃª (trÆ°á»›c thá»i Transformer).
                """)

# ===========================================================
# TAB 4 â€” Deep Learning NER (BiLSTM-CRF pretrained)
# ===========================================================
with tab4:
    st.header("ğŸ§  RÃšT TRÃCH THá»°C THá»‚ â€” HÆ¯á»šNG 2B: Deep Learning (BiLSTM-CRF)")
    st.markdown("""
    MÃ´ hÃ¬nh **BiLSTM-CRF** lÃ  phÆ°Æ¡ng phÃ¡p há»c sÃ¢u truyá»n thá»‘ng cho bÃ i toÃ¡n NER.  
    NÃ³ gá»“m 3 táº§ng chÃ­nh:
    - **Embedding Layer:** há»c biá»ƒu diá»…n tá»« (vÃ­ dá»¥ Word2Vec, FastText)
    - **BiLSTM:** há»c ngá»¯ cáº£nh hai chiá»u (trÆ°á»›câ€“sau)
    - **CRF Layer:** gÃ¡n nhÃ£n chuá»—i tá»‘i Æ°u toÃ n cá»¥c
    
    á» Ä‘Ã¢y, ta sá»­ dá»¥ng model Ä‘Ã£ huáº¥n luyá»‡n sáºµn **`NlpHUST/ner-vietnamese-bilstm-crf`** trÃªn táº­p dá»¯ liá»‡u VLSP tiáº¿ng Viá»‡t.
    """)

    # Lá»±a chá»n phÆ°Æ¡ng thá»©c nháº­p
    mode_bilstm = st.radio(
        "Chá»n phÆ°Æ¡ng thá»©c nháº­p dá»¯ liá»‡u:",
        ["ğŸ“„ Táº£i áº£nh (OCR)", "âœï¸ Nháº­p vÄƒn báº£n"],
        key="mode_bilstm"
    )

    text_bilstm = ""
    if mode_bilstm == "ğŸ“„ Táº£i áº£nh (OCR)":
        uploaded_img_bilstm = st.file_uploader(
            "ğŸ“ Chá»n áº£nh vÄƒn báº£n (JPG/PNG)",
            type=["jpg", "jpeg", "png"],
            key="img_bilstm"
        )
        if uploaded_img_bilstm is not None:
            img_bilstm = Image.open(uploaded_img_bilstm)
            st.image(img_bilstm, caption="ğŸ“œ áº¢nh vÄƒn báº£n Ä‘Æ°á»£c táº£i lÃªn", use_column_width=True)
            with st.spinner("ğŸ”  Äang Ä‘á»c vÄƒn báº£n (OCR)..."):
                text_bilstm = pytesseract.image_to_string(img_bilstm, lang="vie")
    else:
        text_bilstm = st.text_area("âœï¸ Nháº­p ná»™i dung vÄƒn báº£n táº¡i Ä‘Ã¢y:", height=250, key="manual_bilstm")

    # Táº£i pipeline BiLSTM-CRF 1 láº§n duy nháº¥t
    if "bilstm_pipeline" not in st.session_state:
        try:
            with st.spinner("ğŸ”„ Äang táº£i mÃ´ hÃ¬nh BiLSTM-CRF (pretrained) ..."):
                bilstm_model_id = "NlpHUST/ner-vietnamese-electra-base"
                tokenizer_bilstm = AutoTokenizer.from_pretrained(bilstm_model_id, trust_remote_code=True)
                model_bilstm = AutoModelForTokenClassification.from_pretrained(bilstm_model_id, trust_remote_code=True)
                st.session_state.bilstm_pipeline = pipeline(
                    "ner",
                    model=model_bilstm,
                    tokenizer=tokenizer_bilstm,
                    aggregation_strategy="simple"
                )
            st.success("ÄÃ£ táº£i thÃ nh cÃ´ng mÃ´ hÃ¬nh BiLSTM-CRF tiáº¿ng Viá»‡t!")
        except Exception as e:
            st.session_state.bilstm_pipeline = None
            st.error(f"â— KhÃ´ng thá»ƒ táº£i model BiLSTM-CRF tá»« HuggingFace.\nChi tiáº¿t lá»—i: {e}")

    # NÃºt cháº¡y mÃ´ hÃ¬nh
    if st.button("ğŸš€ RÃºt trÃ­ch thá»±c thá»ƒ báº±ng BiLSTM-CRF"):
        if not text_bilstm.strip():
            st.warning("âš ï¸ Vui lÃ²ng táº£i áº£nh hoáº·c nháº­p vÄƒn báº£n trÆ°á»›c.")
        else:
            if st.session_state.bilstm_pipeline is None:
                st.warning("âš ï¸ Model BiLSTM-CRF chÆ°a Ä‘Æ°á»£c táº£i, vui lÃ²ng kiá»ƒm tra láº¡i máº¡ng hoáº·c thá»­ láº¡i sau.")
            else:
                ner_bilstm = st.session_state.bilstm_pipeline
                text_clean = text_bilstm.replace("\n", " ").strip()

                # Chia nhá» vÄƒn báº£n dÃ i
                chunks = []
                while len(text_clean) > 0:
                    chunk = text_clean[:450]
                    end_idx = chunk.rfind(" ")
                    if end_idx == -1:
                        end_idx = len(chunk)
                    chunks.append(text_clean[:end_idx])
                    text_clean = text_clean[end_idx:].strip()

                all_results = []
                for chunk in chunks:
                    results = ner_bilstm(chunk)
                    for r in results:
                        r["chunk"] = chunk
                    all_results.extend(results)

                # TÃ´ mÃ u highlight thá»±c thá»ƒ
                highlighted_text = text_bilstm
                for r in all_results:
                    color = {"PER": "green", "ORG": "orange", "LOC": "purple"}.get(
                        r.get("entity_group", ""), "blue"
                    )
                    highlighted_text = highlighted_text.replace(
                        r["word"],
                        f"<span style='color:{color}; font-weight:bold'>{r['word']}</span>"
                    )

                # Hiá»ƒn thá»‹ káº¿t quáº£
                st.markdown("### ğŸ§© Káº¿t quáº£ trÃ­ch xuáº¥t (BiLSTM-CRF):", unsafe_allow_html=True)
                st.markdown(highlighted_text, unsafe_allow_html=True)

                if all_results:
                    df = pd.DataFrame(all_results)
                    cols = [c for c in ["word", "entity_group", "score"] if c in df.columns]
                    st.dataframe(df[cols], use_container_width=True)


# =========================================
# TAB 5 â€” Transformer-based NER with OCR
# =========================================
with tab5:
    st.header("ğŸ¤– RÃšT TRÃCH THá»°C THá»‚ â€” HÆ¯á»šNG 2C: Transformer PhoBERT")
    st.markdown("""
    Báº¡n cÃ³ thá»ƒ **táº£i áº£nh vÄƒn báº£n phÃ¡p lÃ½ (OCR)** hoáº·c **nháº­p trá»±c tiáº¿p ná»™i dung vÄƒn báº£n** Ä‘á»ƒ mÃ´ hÃ¬nh há»c sÃ¢u PhoBERT tá»± Ä‘á»™ng trÃ­ch xuáº¥t thá»±c thá»ƒ.
    """)

    mode2 = st.radio("Chá»n phÆ°Æ¡ng thá»©c nháº­p dá»¯ liá»‡u:", ["ğŸ“„ Táº£i áº£nh", "âœï¸ Nháº­p vÄƒn báº£n"], key="mode_trans")

    text_input = ""

    if mode2 == "ğŸ“„ Táº£i áº£nh":
        uploaded_img2 = st.file_uploader("ğŸ“ Chá»n áº£nh vÄƒn báº£n (JPG/PNG)", type=["jpg", "jpeg", "png"], key="img_trans")
        if uploaded_img2 is not None:
            img2 = Image.open(uploaded_img2)
            st.image(img2, caption="ğŸ“œ áº¢nh vÄƒn báº£n Ä‘Æ°á»£c táº£i lÃªn", use_column_width=True)
            with st.spinner("ğŸ”  Äang Ä‘á»c vÄƒn báº£n (OCR)..."):
                text_input = pytesseract.image_to_string(img2, lang="vie")
    else:
        text_input = st.text_area("âœï¸ Nháº­p ná»™i dung vÄƒn báº£n táº¡i Ä‘Ã¢y:", height=250, key="manual_text")

    if "ner_pipeline" not in st.session_state:
        with st.spinner("ğŸ”„ Äang táº£i mÃ´ hÃ¬nh Transformer NER (PhoBERT)..."):
            tokenizer = AutoTokenizer.from_pretrained("NlpHUST/ner-vietnamese-electra-base", trust_remote_code=True)
            model = AutoModelForTokenClassification.from_pretrained("NlpHUST/ner-vietnamese-electra-base", trust_remote_code=True)
            st.session_state.ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

    if st.button("ğŸš€ RÃºt trÃ­ch thá»±c thá»ƒ báº±ng Transformer PhoBERT"):
        if not text_input.strip():
            st.warning("âš ï¸ Vui lÃ²ng táº£i áº£nh hoáº·c nháº­p vÄƒn báº£n trÆ°á»›c.")
        else:
            ner = st.session_state.ner_pipeline

            # --- Chia nhá» Ä‘oáº¡n dÃ i ---
            chunks = []
            text_clean = text_input.replace("\n", " ").strip()
            while len(text_clean) > 0:
                chunk = text_clean[:450]
                end_idx = chunk.rfind(" ")
                if end_idx == -1: end_idx = len(chunk)
                chunks.append(text_clean[:end_idx])
                text_clean = text_clean[end_idx:].strip()

            all_results = []
            for chunk in chunks:
                results = ner(chunk)
                for r in results:
                    r["chunk"] = chunk
                all_results.extend(results)

            df = pd.DataFrame(all_results)

            highlighted_text = text_input
            for r in all_results:
                color = {"PER": "green", "ORG": "orange", "LOC": "purple"}.get(r["entity_group"], "blue")
                highlighted_text = highlighted_text.replace(r["word"], f"<span style='color:{color}; font-weight:bold'>{r['word']}</span>")

            st.markdown("### ğŸ§© Káº¿t quáº£ trÃ­ch xuáº¥t (Transformer PhoBERT):", unsafe_allow_html=True)
            st.markdown(highlighted_text, unsafe_allow_html=True)

            if not df.empty:
                st.dataframe(df[["word", "entity_group", "score"]], use_container_width=True)

